{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local torch = require 'torch'\n",
    "require 'hdf5'\n",
    "require 'image'\n",
    "\n",
    "base_path = '/media/wei/DATA/datasets/vlm/'\n",
    "\n",
    "img_size = {}\n",
    "img_size['H'] = 58\n",
    "img_size['W'] = 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600719\t27563\t\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "--loading hdf5 data of training examples\n",
    "dataset_size = 'large'\n",
    "train_file = hdf5.open(base_path .. 'subgestures/ASL_hand_train_' .. dataset_size .. '.hdf5', 'r')\n",
    "X_train = train_file:read('X_train'):all()\n",
    "y_train = train_file:read('y_train'):all()\n",
    "train_file:close()\n",
    "\n",
    "N_train = X_train:size(1)\n",
    "trainset = {};\n",
    "-- resize to a good size for input to NN\n",
    "trainset.data = torch.ByteTensor(N_train,3,img_size['H'],img_size['W']):zero()\n",
    "trainset.label = torch.ByteTensor(N_train,1):zero()\n",
    "function trainset:size() \n",
    "    return self.data:size(1) \n",
    "end\n",
    "for i=1,trainset:size() do \n",
    "  -- trainset.data[i] = image.scale(X_train[i], img_size['H'], img_size['W']):byte()\n",
    "  trainset.data[i] = X_train[i]\n",
    "  trainset.label[i] = y_train[i] + 1\n",
    "end\n",
    "tmp1 = 0\n",
    "tmp2 = 0\n",
    "for i=1,trainset:size() do \n",
    "    if y_train[i] == 1 then\n",
    "        tmp1 = tmp1 + 1\n",
    "    else\n",
    "        tmp2 = tmp2 + 1\n",
    "    end\n",
    "end\n",
    "print(tmp1, tmp2)\n",
    "-- save prepceossed dataset into t7\n",
    "out_path = base_path .. 'subgestures/' .. 'ASL_torch_hand_train_' .. dataset_size .. '.t7'\n",
    "torch.save(out_path, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12501\t13775\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "--Validation set\n",
    "--loading hdf5 data of validation examples\n",
    "val_file = hdf5.open(base_path .. 'subgestures/ASL_hand_val.hdf5', 'r')\n",
    "X_val = val_file:read('X_val'):all()\n",
    "y_val = val_file:read('y_val'):all()\n",
    "val_file:close()\n",
    "\n",
    "N_val = X_val:size(1)\n",
    "\n",
    "-- name validation test to testset for easier programming in torch\n",
    "testset = {};\n",
    "-- resize to a good size for input to NN\n",
    "testset.data = torch.ByteTensor(N_val,3,img_size['H'],img_size['W']):zero()\n",
    "testset.label = torch.ByteTensor(N_val,1):zero()\n",
    "function testset:size() \n",
    "    return self.data:size(1) \n",
    "end\n",
    "for i=1,testset:size() do \n",
    "  --valset.data[i] = image.scale(X_val[i], img_size['H'], img_size['W']):byte()\n",
    "  testset.data[i] = X_val[i]\n",
    "  testset.label[i] = y_val[i] + 1\n",
    "end\n",
    "tmp1 = 0\n",
    "tmp2 = 0\n",
    "for i=1,testset:size() do \n",
    "    if y_val[i] == 1 then\n",
    "        tmp1 = tmp1 + 1\n",
    "    else\n",
    "        tmp2 = tmp2 + 1\n",
    "    end\n",
    "end\n",
    "print(tmp1, tmp2)\n",
    "-- save prepceossed dataset into t7\n",
    "out_path = base_path .. 'subgestures/' .. 'ASL_torch_hand_val.t7'\n",
    "torch.save(out_path, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--Test set\n",
    "--loading hdf5 data of test examples\n",
    "test_file = hdf5.open(base_path .. 'subgestures/ASL_hand_test_gb1113.hdf5', 'r')\n",
    "X_test = test_file:read('X_test'):all()\n",
    "y_test = test_file:read('y_test'):all()\n",
    "test_file:close()\n",
    "\n",
    "N_test = X_test:size(1)\n",
    "testset = {};\n",
    "-- resize to a good size for input to NN\n",
    "testset.data = torch.ByteTensor(N_test,3,img_size['H'],img_size['W']):zero()\n",
    "testset.label = torch.ByteTensor(N_test,1):zero()\n",
    "function testset:size() \n",
    "    return self.data:size(1) \n",
    "end\n",
    "for i=1,testset:size() do \n",
    "  --testset.data[i] = image.scale(X_test[i], img_size['H'], img_size['W']):byte()\n",
    "  testset.data[i] = X_test[i]\n",
    "  testset.label[i] = y_test[i] + 1\n",
    "end\n",
    "-- save prepceossed dataset into t7\n",
    "out_path = base_path .. 'subgestures/' .. 'ASL_torch_hand_test_gb1113.t7'\n",
    "torch.save(out_path, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N_samples:26276\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAIAAABu2d1/AAAKA0lEQVRoge1ZTZMbx3LMzqpG7wArcKkVnxX2zRE++f//mndQ+KKwLFNcggAGPd2V7QOwS1La5Zfop2DEq8vO9qBnEtlZn0jPf/hPPGYaenT9KWPiZ33+c+2M5//3HV/dvjG4/le9mHycKelDIvwnu59mienxGx/08G+M3W8M7l8mhs+1c1z/xtj9xuD+ZWIYGl+w65/sfpp9OHs9Zd8Yu98YXOfo5yvda/98TE+VIP2JB4VaIsnEyx8OCcAXnfkjdq53/Y9A8UFhxdO3CAgEBb0L82seoEvjjyjH0FOv6RFPwh1y+AXxw+JX7TJclyN7FyjwNMHjiXVz+4qwnrLPDmTpCU2fWXyQ75/F9YQ5SUlnx5KUEgGMoadcjU+3nE9t+YrmZPqjTL/AnXvEP0K7AMgEQBoPHJN8spdqj+f6IQns6BTHO3uZv2bi9N4vnn7GRybSAPQejyJe5XzBffbRB23w4moaUrxlt4dyzm4X1V3GBV9K+ZPbHsXK+wbryzL+nze/x/F7cHyq9ftSO1P74NZfZv6w+YwvpYd/3z70fS7jfPeSqxM/dr4cGuJ70f3L4Zq9De/38iXe8b/fgTa386LdVwUEP4rj/IFLSf4nQoW7P+jhcvpngs/8mf0+IVMXp9K4XEnjAfpjQC8X4z7bJyXP/mVMv2X3XWofViRJIHUOc5Ls4es9VBpJANoHX3/GqqE/GYb9XZQpvedeYzxE4reI8RAc7in/R5bMTsrMSRFdvQE1U2cGF0RTQAioQ5AHsd5leSIJhhJiSFDDKFvrsJD3QOvQwEg5kRvtExPlacgWYGDV3ZAMBgAMXM7nHP57YPoQ3KtcVobE7CAMGsiQZ/TersBRiogOjgQAgiGI1AEIGYwARIg4LcrwZjDALfXhTASs72b3EgiTCX+2qvANUIDsZgkYWMEticSzv70AEEPR49CitqXW2hQwSBhjdCzRxqUUP3+F0QBCSEpEACZwWmUmYJhAnuMZO/BUtfmRjO3PzUpZbaZ8lZmJlaEwZceLH74/B9SmqLXu5lprbW1Z2gpAYImIwzIf6+nQ6ila2qxax9xGWBJWTUmDQ1rnDZkkRkRVh87OML4sDfntpkxX03YzbdflqqTr7NPKpkxCiVzlbOYYVEIEJB0bYQmIOvp+3v3v7vXLu9/e1NMSOIbmpdVAKDdwqb1GY2shcAwJRiaRIFzQfY1xQWL38v0gXPSWB6+Y1162eXU95W2xTIdaobm5uZ1ViAwAKhsyIaON2M2bH66nl1N5XQ+/vHy1m6tBUISEpn5qrDUSEjIAHxjCghUJmnW9i/WT2c1sTJmGbLBVdyYKhaOU5FLCgg4MvycA6zIRxBCAUmy93dxkHsd2W1a7Q/11tzvUtrR0qG0HHNVgazgl1taPCkBdCRpv4/ZnwZ226+m6bLbTtLbNNF05r7KVBLUmwMDi2S3nnPMqu5UMygl0JTzntj27DqKT//5v//pyf3j1en83t1Ptb2p7c3fcHfcvd4s0amh3qNKxzwvBXitX+Q9gPq4HR4EYVbXBOtpI1tQBOMKRSIcJfo5AQ2o2kEWsDBwyZSA0qtpNLte3Vz/e3L46nF6+3r16eTdFPF/lZ1tvbdkdTsDdvLRYoVtmno61XkYHZuaFBoSiR3eTNN7pz3HOryEAHgxwBZc4GiGXEscImIkSARKm7gFzURNXTAlkMg2yQ5WdIqKypQHfeuJ6PYlry7XWa2zmVt32rWo+tNpbjb7EjEQAkd6Grk+pIjznjGwpUZ5oUrJxbmkdDicokzwnS/08+NACkmKgCwwIQqLWuSzqKWh5NXF17WXNfDzMBaXGBNjStK/zMXqbaxuNqyvvNjD6paL7pATi61yy5dUql+QZdNDARBiY0tsKYeicEdggKEgJJBqMDogsU/aaTGNlBssZgSLUdmgQsDFeT+V2ezMPdDshegM7hP7x4PUe3O20yTlvzDbMmZ6hDK4Eh5GDAujsQr50nRWVZELiCCE5LjMxLb33UJMx0QYSKsYEgxp7T0NTsqnYZr3Zd9RaJfg7hfKQEvhxV7tdf5fJYixuG7DAXGGAOU1Y0ZOSu5PlPKyr8x5x7ih6Sry3lFDj7KIkYkFHVrqyPKEN0hPJYe4ZyRAA3L2Ndl+jiuTQx9Xrt9MNEMXgxBVQQk4YUhGNdLNzqqBnMyNZUSShR48BtYdxZfaJKZOI3mu01lRrRxskU+sMGeCgZbibR0cy9s50HgBC0qeI1ze5JPQVmNCcomACASNLMoI5OX06wybt2fO/RUSttdZDa633ODcMTjPPTFlVrS71uNS5R4eIsdQeMaSUUs6r6WoqgPp5lPGZ2n2xVe8joqbs7uzJX5/mWttt2VAwpAm5IK05ObPRuCnsg3aQWNTzClpaPcylZMoEngBpRAKmnBPt17scrfSWIhBLRK2jKUEuBNU02oBgSDkApOOH4YKZK9RFin6M0XttbWmt3c0HJ0sqYktpWpecr9Z5NdV6x8SmIDIysr+dWdGcskSy5IwEIZGllGjIwgrMgvVmZqQQv9fpINJHtbsYIngM7Y+vd2/282muvQE4/vSTgVNer9eb2+3t7ffPb7c3m/X62TUK3dxsKiWVQlOpPHA+LQA6QSUr2cAeGJK7GwY1UujzDv5RuKfWFsWhnn692/38yy93+11VkDQ3pjx5v4q2HzhCbxTr+fDjPK6n9XXJJedcVk5TU621aQQ6jI0J5HANjWZAtoGuSJc8PhQRPfq7PZ7uy/XxMXfzRdHUT+qz0NzDV4CQbKxy06gKfzO/mtvPd3flv4u5/0fuN89ufri9ffHddXx3fb1Zq8WpBrOReUkgHVyBVei9J2VPCCqYB7MhX4JNfV8Mg+iCf+ynQXe3saQrn55vxVye3zbBkG1/XOa+nE6neZ7r/tja0paGoTfpdPv9sx9vX/x4+/3+xd9+fH6Taa21zXoNOGktkc6UUsOxSczGYWqUJXO7D4yPaBdA/1iH4WvLJ28qhcmm9XdB67Qm/suPXkevtc7zab9/M8+n02lWbfr173NtL3evCWSkkrD97pqCNLr6AnX3Rh1H2y/90E7e+txr7UuLqL2dlRB/yL0C+Cmudu3F24AEemFutFmAQlUGL6DlXLZ5znObrlur26vZwBXRpaXW1iTJ6HPtAVWwRZvJfeu7etrXQ25jrvVNrfvaj60e5nlutdYGzw/gghchf1y7N3nVgIyE/fG3V3e72oK522owif7y7rce9ebZzQSPOq+t5LLZXl9tV9Pa7LpMpRTIBwjqMNcKvOnx8+vdocX/7H77+0//xd0s4iTM8oN4GphTFqzW2tqIiA452InVJ8zavKhDkRWlY9WxEhbBgGMNIbCvoYhckQLHZaSG8tAL0d95eAKGxgA0tCjmvhx6HHtVO4W0gA35KDakBjQI/vtuor/3I8Hj9n+SD4YUMUZURQAAAABJRU5ErkJggg==",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 58,
       "width": 58
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "valset = torch.load(base_path .. 'subgestures/' .. 'ASL_torch_hand_val.t7')\n",
    "print('N_samples:' .. valset.data:size(1))\n",
    "itorch.image(valset.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- save model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
